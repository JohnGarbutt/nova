#!/usr/bin/python26 -u

# Copyright (c) 2012 OpenStack, LLC
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Handle the uploading of images via Swift.

Requires python2.6.

"""

import hashlib
import httplib
import math
import os

import utils

import pluginlib_nova
import swift_client

logging = pluginlib_nova.logging
PluginError = pluginlib_nova.PluginError

MEGABYTES_TO_BYTES_FACTOR = 1024 * 1024
FIVE_GIGABYTES = 5120 * MEGABYTES_TO_BYTES_FACTOR
TWO_HUNDRED_MEGABYTES = 200 * MEGABYTES_TO_BYTES_FACTOR

# TODO(johngarbutt) remove once we test experimental reader
USE_BUFFERED_READER = False


class VHDsTooLargeError(Exception):
    pass


class ChunkReader(object):
    def __init__(self, fd, checksum, total):
        self.fd = fd
        self.checksum = checksum
        self.total = total
        self.bytes_read = 0

    def read(self, i):
        left = self.total - self.bytes_read
        if i > left:
            i = left
        result = self.fd.read(i)
        self.bytes_read += len(result)
        self.checksum.update(result)
        return result


class BufferedReader(object):
    """Buffer a chunk (segment) worth of data before sending it swift.  This
    creates the ability to back the input stream up and re-try put object
    requests.  (Swiftclient will try to reset the file pointer on any upload
    failure if seek and tell methods are provided on the input file.)

    NOTE:
    This reader will consume memory proportional to:
    segment size * number of in-flight upload requests
    """

    def __init__(self, fd, initial_checksum, total):
        self.fd = fd
        self._initial_checksum = initial_checksum
        self.checksum = initial_checksum.copy()
        self.total = total

        # TODO(johngarbutt) I think we can re-read from disk
        # to save on the memory usage
        self.buf = ""
        self.offset = 0

    def read(self, sz):
        # Maintain a single chunk's worth of buffered data from the source fd.
        remaining = self.total - self.offset
        if sz > remaining:
            sz = remaining

        self._buffer()
        return self._read(sz)

    def _buffer(self):
        # buffer as much as the remainder of the chunk
        # (will likely return less data, as sockets are in non-blocking mode)
        remaining = self.total - len(self.buf)
        buf = self.fd.read(remaining)
        self.buf += buf

    def _read(self, request_sz):
        # read out of the buffered chunk

        # handle the case where buffer didn't necessarily buffer enough data.
        # this is possible with nonblocking fd reads
        available = len(self.buf) - self.offset
        request_sz = min(request_sz, available)

        end = min(self.offset + request_sz, self.total)
        result = self.buf[self.offset:end]
        self.offset = end

        self.checksum.update(result)
        return result

    # NOTE(belliott) seek and tell get used by python-swiftclient to "reset"
    # if there is a put_object error
    def seek(self, offset, checksum_reset_sz=65536):
        # backup the stream to 'offset' and re-generate the checksum
        logging.debug("Seek from %s to %s" % (self.offset, offset))
        self.offset = offset

        # backing up, re-calculate the checksum
        self.checksum = self._initial_checksum
        start = 0
        while start < offset:
            end = min(start + checksum_reset_sz, offset)
            self.checksum.update(self.buf[start:end])
            start = end

    def tell(self):
        return self.offset


def hashfile(afile, hasher, blocksize=65536):
    buf = afile.read(blocksize)
    while len(buf) > 0:
        hasher.update(buf)
        buf = afile.read(blocksize)
    return hasher.hexdigest()


def _create_container_if_missing(swift_conn, container):
    """Creates a missing container in Swift if the
    ``swift_store_create_container_on_put`` option is set.

    :param container: Name of container to create
    :param swift_conn: Connection to Swift
    """
    try:
        swift_conn.head_container(container)
    except swift_client.ClientException, e:
        if e.http_status == httplib.NOT_FOUND:
            try:
                logging.debug("Adding container: %s" % container)
                swift_conn.put_container(container)
            except swift_client.ClientException, e:
                msg = ("Failed to add container to Swift.\n"
                       "Got error from Swift: %s" % e)
                logging.error(msg)
                raise
        else:
            msg = "Received unexpected status from the swift_client %s " % e
            logging.error(msg)
            raise


def _make_swift_connection(**params):
    """Creates a connection using the Swift client library.

    :param auth_url The authentication for v1 style Swift auth or
                    v2 style Keystone auth.
    :param user A string containing the tenant:user information.
    :param key  A string containing the key/password for the connection.
    :param storage_url A string containing the storage URL.
    :param token A string containing the token
    """
    snet = params.get('swift_enable_snet', False)

    # Single Tenant
    auth_url = params.get('full_auth_address')
    auth_version = params.get('swift_store_auth_version')
    user = params.get('swift_store_user')
    key = params.get('swift_store_key')

    # Multi Tenant
    storage_url = params.get('storage_url')
    token = params.get('token')

    # Optional
    os_options = {}
    if 'region_name' in params:
        os_options['region_name'] = params['region_name']

    full_auth_url = (auth_url if not auth_url or auth_url.endswith('/')
                     else auth_url + '/')
    logging.debug("Creating Swift connection with "
                  "(auth_address=%(full_auth_url)s, user=%(user)s, "
                  "snet=%(snet)s, auth_version=%(auth_version)s)" %
                  locals())

    if token is not None:
        #NOTE(ramielrowe): multi-tenant supports v2 auth only
        return swift_client.Connection(
            None, user, None, preauthurl=storage_url, preauthtoken=token,
            snet=snet, auth_version='2')
    else:
        return swift_client.Connection(full_auth_url, user, key, snet=snet,
                                       auth_version=auth_version,
                                       os_options=os_options)


def _get_reader(tar_file, checksum, chunk_size):
    if USE_BUFFERED_READER:
        return BufferedReader(tar_file, checksum, chunk_size)
    else:
        return ChunkReader(tar_file, checksum, chunk_size)


def _upload_tarball_to_swift(swift_conn, obj_name, tar_file, undo_chunks,
                             tar_size=0, large_object_size=FIVE_GIGABYTES,
                             large_object_chunk_size=TWO_HUNDRED_MEGABYTES,
                             project_id=None, container='images'):
    """Uploads the image tarball to swift.

    :param swift_conn Swift connection object
    :param obj_name Image id of the image to be uploaded
    :param tar_file The tarballed image data to write
    :param tar_size The size of the image data to write
    :param large_object_size The maximum image file size in megabytes that the
                             file can be uploaded without chunking.
                             Default is 5 gigabyte.
    :param large_object_chunk_size The size of chunks in megabytes in which
                                   images are uploaded to swift.
                                   Default is 200 megabyte.
    :param container Container that the account should use.
    """

    # Write the image into Swift in chunks.
    chunk_id = 1
    total_chunks = '?'
    if tar_size > 0:
        total_chunks = str(int(
            math.ceil(float(tar_size) /
                      float(large_object_chunk_size))))
    else:
        # tar_size == 0 is when we don't know the size
        # of the image. This can occur with older clients
        # that don't inspect the payload size.
        logging.debug("Cannot determine image size. Adding as a "
                      "segmented object to Swift.")

    checksum = hashlib.md5()
    combined_chunks_size = 0
    while True:
        chunk_size = large_object_chunk_size
        if tar_size == 0:
            content_length = None
        else:
            left = tar_size - combined_chunks_size
            if left == 0:
                break
            if chunk_size > left:
                chunk_size = left
            content_length = chunk_size

        chunk_name = "%s-%05d" % (obj_name, chunk_id)
        reader = _get_reader(tar_file, checksum, chunk_size)
        try:
            chunk_etag = swift_conn.put_object(container, chunk_name, reader,
                                               content_length=content_length)
        except swift_client.ClientException, e:
            msg = "Failed to upload image chunk %s: %s" % (chunk_name, e)
            logging.error(msg)
            raise

        bytes_read = reader.bytes_read
        msg = ("Wrote chunk %(chunk_name)s (%(chunk_id)d/"
               "%(total_chunks)s) of length %(bytes_read)d "
               "to Swift returning MD5 of content: "
               "%(chunk_etag)s")
        logging.debug(msg % locals())

        if bytes_read == 0:
            # Delete the last chunk, because it's of zero size.
            # This will happen if image_size == 0.
            logging.debug("Deleting final zero-length chunk")
            try:
                swift_conn.delete_object(container, chunk_name)
            except swift_client.ClientException, e:
                msg = "Failed to delete zero length chunk %s" % e
                logging.warning(msg)

            break

        # remember what we have uploaded already,
        # so we can delete it on failure
        undo_chunks.append((container, chunk_name))

        chunk_id += 1
        combined_chunks_size += bytes_read

    # In the case we have been given an unknown image size,
    # set the image_size to the total size of the combined chunks.
    if tar_size == 0:
        tar_size = combined_chunks_size

    # Now we write the object manifest and return the
    # manifest's etag...
    manifest = "%s/%s" % (container, obj_name)
    headers = {'ETag': hashlib.md5("").hexdigest(),
               'X-Object-Manifest': manifest}
    # NOTE(ameade): the tenant id is only required because server-side calls
    #                 to rackspace auth require it
    if project_id:
        headers['x-tenant-id'] = project_id

    # The ETag returned for the manifest is actually the
    # MD5 hash of the concatenated checksums of the strings
    # of each chunk. So, we ignore this result in favour of
    # the MD5 of the entire image file contents, so that
    # users can verify the image file contents accordingly
    try:
        swift_conn.put_object(container, obj_name,
                              None, headers=headers)
    except swift_client.ClientException, e:
        msg = "Failed to upload object manifest %s" % e
        logging.error(msg)
        raise

    obj_etag = checksum.hexdigest()

    return obj_etag


def _upload_tarball(tar_path, obj_name, undo_chunks, **params):
    """Create a tarball of the image and then stream that into swift
    using swift_client's chunked-transfer-encoded HTTP.
    """

    swift_conn = _make_swift_connection(**params)

    container = params['swift_store_container']
    create_container_on_put = params['swift_store_create_container_on_put']
    if create_container_on_put:
        _create_container_if_missing(swift_conn, container)

    project_id = params.get('project_id')
    large_object_size = params['swift_store_large_object_size']
    large_object_size_bytes = large_object_size * MEGABYTES_TO_BYTES_FACTOR
    large_object_chunk_size = params['swift_store_large_object_chunk_size']
    large_object_chunk_size_bytes = large_object_chunk_size * \
        MEGABYTES_TO_BYTES_FACTOR
    tar_size = os.path.getsize(tar_path)

    with open(tar_path, 'r') as tar_file:
        etag = _upload_tarball_to_swift(swift_conn, obj_name, tar_file,
                undo_chunks, tar_size=tar_size,
                large_object_size=large_object_size_bytes,
                large_object_chunk_size=large_object_chunk_size_bytes,
                container=container, project_id=project_id)

    return etag, tar_size


def _cleanup_after_failed_upload(uploaded_chunks, **params):
    try:
        logging.debug("Attempting to cleanup after failed upload.")
        swift_conn = _make_swift_connection(**params)
        for container, obj_name in uploaded_chunks:
            logging.debug("Delete object: %s" % obj_name)
            swift_conn.delete_object(container, obj_name)
    except Exception, e:
        # skip errors so we can retry
        msg = "Failed to cleanup after failed upload: %s" % e
        logging.warning(msg)


def _upload_tarball_with_retry(tar_path, obj_name, **params):
    last_exception = None

    for attempt in [0, 1, 2]:
        uploaded_chunks = []
        try:
            return _upload_tarball(tar_path, obj_name, uploaded_chunks,
                                   **params)
        except Exception, e:
            last_exception = e
            msg = "Image upload attempt %s failed due to: %s" \
                % (attempt, last_exception)
            logging.exception(msg)

            _cleanup_after_failed_upload(uploaded_chunks, **params)

    msg = "Unable to upload after three attempts: %s" % last_exception
    logging.error(msg)
    raise PluginError(msg)


def _create_tar_file(staging_path, tar_path):
    # TODO(johnthetubaguy) we need to stream this into swift
    # to avoid running out of disk space, and to reduce I/O load
    with open(tar_path, 'w') as tar_file:
        utils.create_tarball(tar_file, staging_path)


def upload_vhd(session, vdi_uuids, sr_path, image_id, **params):
    """Bundle the VHDs comprising an image and then stream them into Swift.
    """
    staging_path = utils.make_staging_area(sr_path)
    swift_fileobj_path = staging_path + '_swift'
    utils.make_dir(swift_fileobj_path)

    try:
        size = utils.prepare_staging_area(sr_path, staging_path, vdi_uuids)

        max_size = params['max_size']
        if max_size > 0 and size > max_size:
            logging.warn("The size of vhds (%d) exceeds the maximum snapshot"
                         " size (%d) ", size, max_size)
            raise VHDsTooLargeError(str(size))

        obj_name = str(image_id)
        tar_path = '%s/%s' % (swift_fileobj_path, obj_name)
        _create_tar_file(staging_path, tar_path)

        etag, image_size = _upload_tarball_with_retry(tar_path, obj_name,
                                                      **params)
    finally:
        utils.cleanup_staging_area(staging_path)
        utils.cleanup_staging_area(swift_fileobj_path)

    return {'etag': etag,
            'image_size': image_size,
            'disk_format': 'vhd',
            'container_format': 'ovf'}


if __name__ == '__main__':
    pluginlib_nova.configure_logging('swift')
    utils.register_plugin_calls(upload_vhd)
